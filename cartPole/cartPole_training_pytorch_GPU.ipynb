{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# REINFORCE Cart Pole Pytorch"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "print(sys.version)\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Initialization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "env = gym.make('CartPole-v0')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class policy_estimator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(policy_estimator, self).__init__()\n",
    "        self.n_inputs = env.observation_space.shape[0]\n",
    "        self.n_outputs = env.action_space.n\n",
    "\n",
    "        # Define network\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(self.n_inputs, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, self.n_outputs),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def predict(self, state):\n",
    "        action_probs = self.network(torch.FloatTensor(state).to(device=device))\n",
    "\n",
    "        return action_probs\n",
    "\n",
    "\n",
    "s = env.reset()\n",
    "pe = policy_estimator().to(device=device)\n",
    "\n",
    "print(pe.predict(s))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Discount reward function"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def discount_rewards(rewards, gamma=0.99):\n",
    "    r = np.array([gamma**i * rewards[i] for i in range(len(rewards))])\n",
    "\n",
    "    # Reverse the array direction for cumsum and then\n",
    "    # revert back to the original order\n",
    "    r = r[::-1].cumsum()[::-1]\n",
    "\n",
    "    return (r - r.mean()) #/ (r.std() + np.finfo(np.float32).eps)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sample Action"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def sample_action(action_space_n, probs):\n",
    "    ran_num = np.random.random()\n",
    "\n",
    "    p = 0.0\n",
    "    for i in range(action_space_n):\n",
    "        p += probs[i].item()\n",
    "\n",
    "        if ran_num < p:\n",
    "            return i"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The REINFORCE algorithm"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def REINFORCE(env, policy_estimator, num_episodes=5000, batch_size=10, gamma=0.99, learning_rate=0.01):\n",
    "\n",
    "    # Set up lists to hold results\n",
    "    total_rewards = []\n",
    "    batch_rewards = []\n",
    "    batch_actions =[]\n",
    "    batch_states = []\n",
    "    batch_counter = 1\n",
    "\n",
    "    #Define Optimizer\n",
    "    optimizer = optim.Adam(policy_estimator.network.parameters(), lr=learning_rate)\n",
    "\n",
    "    action_space = np.arange(env.action_space.n)\n",
    "\n",
    "    for ep in range(num_episodes):\n",
    "        s_0 = env.reset()\n",
    "        states = []\n",
    "        rewards = []\n",
    "        actions = []\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            #Get actions and convert to numpy array\n",
    "\n",
    "            #action_probs = policy_estimator.predict(s_0).cpu().detach().numpy()\n",
    "            #action = np.random.choice(action_space, p=action_probs)\n",
    "\n",
    "            action_probs = policy_estimator.predict(s_0)\n",
    "            action = sample_action(env.action_space.n, action_probs)\n",
    "\n",
    "            s_1, r, done, _ = env.step(action)\n",
    "\n",
    "            states.append(s_0)\n",
    "            rewards.append(r)\n",
    "            actions.append(action)\n",
    "            s_0 = s_1\n",
    "\n",
    "            # If done, batch data\n",
    "            if done:\n",
    "                batch_rewards.extend(discount_rewards(rewards, gamma))\n",
    "                batch_states.extend(states)\n",
    "                batch_actions.extend(actions)\n",
    "                batch_counter += 1\n",
    "                total_rewards.append(sum(rewards))\n",
    "\n",
    "                # If batch is complete, update network\n",
    "                if batch_counter == batch_size:\n",
    "                    optimizer.zero_grad()\n",
    "                    state_tensor = torch.FloatTensor(batch_states)\n",
    "                    reward_tensor = torch.FloatTensor(batch_rewards).to(device=device)\n",
    "                    # Actions are used as indices, must be LongTensor\n",
    "                    action_tensor = torch.LongTensor(batch_actions)\n",
    "\n",
    "                    # Calculate loss\n",
    "                    logprob = torch.log(policy_estimator.predict(state_tensor)).to(device=device)\n",
    "                    selected_logprobs = reward_tensor * logprob[np.arange(len(action_tensor)), action_tensor]\n",
    "                    loss = -selected_logprobs.mean()\n",
    "\n",
    "                    # Calculate gradients\n",
    "                    loss.backward()\n",
    "                    # Apply gradients\n",
    "                    optimizer.step()\n",
    "\n",
    "                    batch_rewards = []\n",
    "                    batch_actions = []\n",
    "                    batch_states = []\n",
    "                    batch_counter = 1\n",
    "\n",
    "                # Print running average\n",
    "                print(\"\\rEp: {} Average of last 10: {:.2f}\".format(ep + 1, np.mean(total_rewards[-10:])), end=\"\")\n",
    "\n",
    "    return total_rewards"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Results"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "rewards = REINFORCE(env, pe)\n",
    "window = 10\n",
    "smoothed_rewards = [np.mean(rewards[i-window:i+1]) if i > window \n",
    "                    else np.mean(rewards[:i+1]) for i in range(len(rewards))]\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(rewards)\n",
    "plt.plot(smoothed_rewards)\n",
    "plt.ylabel('Total Rewards')\n",
    "plt.xlabel('Episodes')\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Save model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "torch.save(pe, 'cartPole_model.pt')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}